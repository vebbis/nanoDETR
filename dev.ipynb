{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "2f463215-065c-469d-bbd1-17d6531cac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms as T\n",
    "from torchvision.datasets import wrap_dataset_for_transforms_v2\n",
    "from torchvision.models import ResNet50_Weights  # <-- import this\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3305e856-353f-46c8-9d01-36a2b5e0b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(weights = ResNet50_Weights.IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2b912e66-1f05-4996-b2be-91d193fe0817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('horse', 'person')"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# haven't checked if these are official. website down\n",
    "itol = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "itol[13-1], itol[15-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3cefa7cb-79ba-4229-89ff-d883977678b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(sample):\n",
    "    # img is normalized, so have to unnormalize\n",
    "    mean= torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std=torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    img, target = sample\n",
    "    img = img.data\n",
    "    img = img*std + mean\n",
    "    toimg = v2.ToPILImage()\n",
    "    labels = [str(itol[i-1]) for i in target['labels']]\n",
    "    toimg(draw_bounding_boxes(img, target['boxes'].data, width = 3, labels = labels)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f85d9e94-90b9-4bcb-83b3-c9aa690d97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet stats here: https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights\n",
    "transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "data_filepath = '/Users/veb/ms/nanoDETR/data'\n",
    "dataset = datasets.VOCDetection(root = data_filepath, \n",
    "                                year = '2012', \n",
    "                                image_set = 'train', \n",
    "                                download = False,\n",
    "                                transform = transform) # len 5717, consisten with data\n",
    "dataset = wrap_dataset_for_transforms_v2(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "00aa2891-de46-40a5-8edc-5a270619b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can now put through resnet\n",
    "# original img: (3, H0, W0)\n",
    "# backbone output: (C, H, W), where typically C = 2048, H, W = H0 / 32, W0 / 32\n",
    "\n",
    "# The input images are batched together, applying 0-padding adequately to ensure\n",
    "# they all have the same dimensions (H0,W0) as the largest image of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6725a1a3-1e7e-44b5-b701-dc31766c6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5550)\n",
    "in_channels = 2048\n",
    "hidden_dim = 256 # = d_model in AttentionIsAllYouNeed\n",
    "img, target = dataset[0]\n",
    "\n",
    "\n",
    "backbone = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "downsample = nn.Conv2d(in_channels, hidden_dim, kernel_size = 1, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = backbone(img.unsqueeze(0))    \n",
    "assert out.requires_grad == False\n",
    "\n",
    "down = downsample(out) # (B, 2048, 14, 16) -> (B, hidden_dim, 14, 16) \n",
    "    \n",
    "flattened = down.flatten(2) # (B, hidden_dim, H*W) \n",
    "flattened = flattened.permute(0,2,1) # (B, H*W, hidden_dim) \n",
    "\n",
    "\n",
    "# learnable positional embeddings, C = hidden_dim\n",
    "B,HW,C = flattened.shape\n",
    "pos_embed = nn.Parameter(torch.randn(size = (1, HW, hidden_dim), dtype = flattened.dtype, requires_grad = True))\n",
    "flattened.shape, pos_embed.shape\n",
    "x = flattened + pos_embed # (B, H*W, hidden_dim) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "09ba2393-8162-4d4c-a472-3ab4cfcd68b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4047, grad_fn=<StdBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 224, 256]),\n",
       " torch.Size([1, 256, 224]),\n",
       " torch.Size([1, 224, 224]),\n",
       " <function Tensor.std>)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention head\n",
    "torch.manual_seed(5550)\n",
    "\n",
    "query = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "key = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "value = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "# project into seperate spaces\n",
    "q = query(x) # (B, H*W, hidden_dim) \n",
    "k = key(x)\n",
    "v = value(x)\n",
    "\n",
    "# attention scores\n",
    "scores = q @ k.transpose(-2,-1)\n",
    "scores /= math.sqrt(hidden_dim) # print(scores.std()) will be ish 0.4 => breaks gaussian assumption\n",
    "scores = F.softmax(scores, dim = -1)\n",
    "out = scores @ v\n",
    "\n",
    "\n",
    "q.shape, k.transpose(-2,-1).shape, scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c613baa-a7b5-4a65-9e78-baf566b7b630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
