{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "2f463215-065c-469d-bbd1-17d6531cac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms as T\n",
    "from torchvision.datasets import wrap_dataset_for_transforms_v2\n",
    "from torchvision.models import ResNet50_Weights  # <-- import this\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3305e856-353f-46c8-9d01-36a2b5e0b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(weights = ResNet50_Weights.IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2b912e66-1f05-4996-b2be-91d193fe0817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('horse', 'person')"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# haven't checked if these are official. website down\n",
    "itol = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "itol[13-1], itol[15-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3cefa7cb-79ba-4229-89ff-d883977678b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(sample):\n",
    "    # img is normalized, so have to unnormalize\n",
    "    mean= torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std=torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    img, target = sample\n",
    "    img = img.data\n",
    "    img = img*std + mean\n",
    "    toimg = v2.ToPILImage()\n",
    "    labels = [str(itol[i-1]) for i in target['labels']]\n",
    "    toimg(draw_bounding_boxes(img, target['boxes'].data, width = 3, labels = labels)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f85d9e94-90b9-4bcb-83b3-c9aa690d97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet stats here: https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights\n",
    "transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "data_filepath = '/Users/veb/ms/nanoDETR/data'\n",
    "dataset = datasets.VOCDetection(root = data_filepath, \n",
    "                                year = '2012', \n",
    "                                image_set = 'train', \n",
    "                                download = False,\n",
    "                                transform = transform) # len 5717, consisten with data\n",
    "dataset = wrap_dataset_for_transforms_v2(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "00aa2891-de46-40a5-8edc-5a270619b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can now put through resnet\n",
    "# original img: (3, H0, W0)\n",
    "# backbone output: (C, H, W), where typically C = 2048, H, W = H0 / 32, W0 / 32\n",
    "\n",
    "# The input images are batched together, applying 0-padding adequately to ensure\n",
    "# they all have the same dimensions (H0,W0) as the largest image of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "6725a1a3-1e7e-44b5-b701-dc31766c6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5550)\n",
    "in_channels = 2048\n",
    "hidden_dim = 256 # = d_model in AttentionIsAllYouNeed\n",
    "img, target = dataset[0]\n",
    "\n",
    "\n",
    "backbone = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "downsample = nn.Conv2d(in_channels, hidden_dim, kernel_size = 1, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = backbone(img.unsqueeze(0))    \n",
    "assert out.requires_grad == False\n",
    "\n",
    "down = downsample(out) # (B, 2048, 14, 16) -> (B, hidden_dim, 14, 16) \n",
    "    \n",
    "flattened = down.flatten(2) # (B, hidden_dim, H*W) \n",
    "flattened = flattened.permute(0,2,1) # (B, H*W, hidden_dim) \n",
    "\n",
    "\n",
    "# learnable positional embeddings, C = hidden_dim\n",
    "B,HW,C = flattened.shape\n",
    "pos_embed = nn.Parameter(torch.randn(size = (1, HW, hidden_dim), dtype = flattened.dtype, requires_grad = True))\n",
    "flattened.shape, pos_embed.shape\n",
    "x = flattened + pos_embed # (B, H*W, hidden_dim) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "09ba2393-8162-4d4c-a472-3ab4cfcd68b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4047, grad_fn=<StdBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 224, 256]),\n",
       " torch.Size([1, 256, 224]),\n",
       " torch.Size([1, 224, 224]),\n",
       " <function Tensor.std>)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention head\n",
    "torch.manual_seed(5550)\n",
    "\n",
    "query = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "key = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "value = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "# project into seperate spaces\n",
    "q = query(x) # (B, H*W, hidden_dim) \n",
    "k = key(x)\n",
    "v = value(x)\n",
    "\n",
    "# attention scores\n",
    "scores = q @ k.transpose(-2,-1)\n",
    "scores /= math.sqrt(hidden_dim) # print(scores.std()) will be ish 0.4 => breaks gaussian assumption\n",
    "scores = F.softmax(scores, dim = -1)\n",
    "out = scores @ v\n",
    "\n",
    "\n",
    "q.shape, k.transpose(-2,-1).shape, scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "1c613baa-a7b5-4a65-9e78-baf566b7b630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 224, 256]), torch.Size([1, 224, 256]))"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5550)\n",
    "nhead = 8\n",
    "hidden_dim = 256 # = d_model in AttentionIsAllYouNeed\n",
    "\n",
    "\n",
    "class EncoderHead(nn.Module):\n",
    "    def __init__(self, positional_encoding, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.head_size = hidden_dim // nhead # 32\n",
    "        self.query = nn.Linear(hidden_dim, self.head_size, bias = False) # 256 -> 32\n",
    "        self.key = nn.Linear(hidden_dim, self.head_size, bias = False)\n",
    "        self.value = nn.Linear(hidden_dim, self.head_size, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # project into separate spaces\n",
    "        q = self.query(x + self.positional_encoding) # (B, H*W, head_size) \n",
    "        k = self.key(x + self.positional_encoding)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # attention scores\n",
    "        scores = q @ k.transpose(-2,-1)\n",
    "        scores /= math.sqrt(self.head_size) # print(scores.std()) will be ish 0.4 => breaks gaussian assumption\n",
    "        scores = F.softmax(scores, dim = -1)\n",
    "        out = scores @ v # (B, H*W, head_size) \n",
    "\n",
    "        return out\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, pos_encode, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.encoder_heads = nn.ModuleList([EncoderHead(pos_encode) for _ in range(nhead)])\n",
    "        self.cat_proj = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cat = torch.cat([head(x) for head in self.encoder_heads], dim = -1) # (B, HW, head_size) -> (B, HW, hidden_dim)\n",
    "        out = self.cat_proj(cat)\n",
    "        return out\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, pos_encode, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.pos_encode = pos_encode\n",
    "        self.encoder_layer = EncoderLayer(self.pos_encode)\n",
    "\n",
    "        # layer norm\n",
    "        self.gamma = nn.Parameter(torch.ones(1, 1, hidden_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        # ffn\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # attention layer\n",
    "        x_res = x\n",
    "        mean = x_res.mean(dim = -1, keepdim = True) # for layernorm\n",
    "        var = x_res.var(dim = -1, correction = 0, keepdim = True)\n",
    "        x_res = x_res - mean\n",
    "        x_res = x_res / torch.sqrt(var + self.eps)\n",
    "        x_res = x_res*self.gamma + self.beta\n",
    "        x_res = self.encoder_layer(x_res) # (B, HW, hidden_dim)\n",
    "        x = x + x_res # residual connection\n",
    "\n",
    "        # compute layer\n",
    "        x_res = x\n",
    "        x_res = self.layer_norm(x_res)\n",
    "        x_res = self.fc1(x_res)\n",
    "        x_res = self.relu(x_res)\n",
    "        x_res = self.fc2(x_res)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, pos_encode, num_encoder_layers = 6, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderBlock(pos_encode) for _ in range(num_encoder_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        \n",
    "pos_encode = nn.Parameter(torch.randn(size = (1, HW, hidden_dim), dtype = flattened.dtype))\n",
    "elayer = EncoderLayer(pos_encode)\n",
    "block = EncoderBlock(pos_encode)\n",
    "encoder = Encoder(pos_encode)\n",
    "encoder(x).shape, x.shape\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "e6adaf7c-ead1-4027-b90d-b2f283b44378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1267, -1.4871, -1.0262, -0.6905],\n",
      "         [-0.7764,  0.7584, -0.2931,  0.2300]]])\n",
      "tensor([[[ 1.1267, -1.4871],\n",
      "         [-0.7764,  0.7584]]])\n",
      "tensor([[[-1.0262, -0.6905],\n",
      "         [-0.2931,  0.2300]]])\n",
      "torch.Size([2, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(1,2,4)\n",
    "print(t)\n",
    "t = t.split(2, dim = -1)\n",
    "# t[1].shape, len(t)\n",
    "print(t[0])\n",
    "print(t[1])\n",
    "print(torch.stack(t).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "d8b052b8-15e3-4b08-92b6-c0725b07a42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 224, 256])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MHAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead, QK_positional_encoding):\n",
    "        super().__init__()\n",
    "        self.nhead = nhead\n",
    "        self.head_size = embed_dim // nhead\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.pos_encoding = QK_positional_encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, HW, C = x.shape\n",
    "        \n",
    "        q = self.query(x + self.pos_encoding)\n",
    "        k = self.key(x + self.pos_encoding)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # TODO: go over pen and paper for reshaping q, k, v\n",
    "        q = q.view(B, HW, self.nhead, self.head_size).transpose(1,2) # (B, nhead, HW, head_size)\n",
    "        k = k.view(B, HW, self.nhead, self.head_size).transpose(1,2)\n",
    "        v = v.view(B, HW, self.nhead, self.head_size).transpose(1,2)\n",
    "        \n",
    "        \n",
    "        scores = q @ k.transpose(-2,-1) # (B, nhead, HW, head_size) @ (B, nhead, head_size, HW) ---> # (B, nhead, HW, HW)\n",
    "        scores /= math.sqrt(self.head_size)\n",
    "        scores = F.softmax(scores, dim = -1)\n",
    "        out = scores @ v\n",
    "        out = out.transpose(1,2).flatten(2)\n",
    "        out = self.projection(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "pos_encode = nn.Parameter(torch.randn(size = (1, HW, hidden_dim), dtype = flattened.dtype))\n",
    "mhattn = MHAttention(256, 8, pos_encode)\n",
    "mhattn(x).shape\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "ee336c00-4e73-4c99-9c4c-e96a6adf3215",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3786168963.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[441]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# building the decoder\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, memory, memory_pos_encoding, query_pos_encoding, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.memory = memory\n",
    "        self.memory_pos_encoding = memory_pos_encoding\n",
    "        self.query_pos_encoding = query_pos_encoding\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.multihead_selfattention = nn.MultiheadAttention(embed_dim = hidden_dim, num_heads = nhead)\n",
    "        self.layer_norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.multihead_crossattention = nn.MultiheadAttention(embed_dim = hidden_dim, num_heads = nhead)\n",
    "        self.layer_norm_3 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = self.layer_norm(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d8d1f-b06a-42ff-b638-71189aefe714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
