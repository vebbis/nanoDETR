{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f463215-065c-469d-bbd1-17d6531cac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms as T\n",
    "from torchvision.datasets import wrap_dataset_for_transforms_v2\n",
    "from torchvision.models import ResNet50_Weights  # <-- import this\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3305e856-353f-46c8-9d01-36a2b5e0b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(weights = ResNet50_Weights.IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b912e66-1f05-4996-b2be-91d193fe0817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('horse', 'person')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# haven't checked if these are official. website down\n",
    "itol = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "itol[13-1], itol[15-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cefa7cb-79ba-4229-89ff-d883977678b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(sample):\n",
    "    # img is normalized, so have to unnormalize\n",
    "    mean= torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std=torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    img, target = sample\n",
    "    img = img.data\n",
    "    img = img*std + mean\n",
    "    toimg = v2.ToPILImage()\n",
    "    labels = [str(itol[i-1]) for i in target['labels']]\n",
    "    toimg(draw_bounding_boxes(img, target['boxes'].data, width = 3, labels = labels)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f85d9e94-90b9-4bcb-83b3-c9aa690d97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet stats here: https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights\n",
    "transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "data_filepath = '/Users/veb/ms/nanoDETR/data'\n",
    "dataset = datasets.VOCDetection(root = data_filepath, \n",
    "                                year = '2012', \n",
    "                                image_set = 'train', \n",
    "                                download = False,\n",
    "                                transform = transform) # len 5717, consisten with data\n",
    "dataset = wrap_dataset_for_transforms_v2(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00aa2891-de46-40a5-8edc-5a270619b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can now put through resnet\n",
    "# original img: (3, H0, W0)\n",
    "# backbone output: (C, H, W), where typically C = 2048, H, W = H0 / 32, W0 / 32\n",
    "\n",
    "# The input images are batched together, applying 0-padding adequately to ensure\n",
    "# they all have the same dimensions (H0,W0) as the largest image of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23b1a0a0-d864-4d86-a5f2-db214492661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 442, 500])\n"
     ]
    }
   ],
   "source": [
    "img, target = dataset[0]\n",
    "print(img.shape)\n",
    "\n",
    "utils.v2show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6725a1a3-1e7e-44b5-b701-dc31766c6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5550)\n",
    "in_channels = 2048\n",
    "hidden_dim = 256 # = d_model in AttentionIsAllYouNeed\n",
    "img, target = dataset[0]\n",
    "\n",
    "\n",
    "backbone = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "downsample = nn.Conv2d(in_channels, hidden_dim, kernel_size = 1, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = backbone(img.unsqueeze(0))    \n",
    "assert out.requires_grad == False\n",
    "\n",
    "down = downsample(out) # (B, 2048, 14, 16) -> (B, hidden_dim, 14, 16) \n",
    "    \n",
    "flattened = down.flatten(2) # (B, hidden_dim, H*W) \n",
    "flattened = flattened.permute(0,2,1) # (B, H*W, hidden_dim) \n",
    "\n",
    "\n",
    "# learnable positional embeddings, C = hidden_dim\n",
    "B,HW,C = flattened.shape\n",
    "pos_embed = nn.Parameter(torch.randn(size = (1, HW, hidden_dim), dtype = flattened.dtype, requires_grad = True))\n",
    "flattened.shape, pos_embed.shape\n",
    "x = flattened + pos_embed # (B, H*W, hidden_dim) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09ba2393-8162-4d4c-a472-3ab4cfcd68b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 224, 256]),\n",
       " torch.Size([1, 256, 224]),\n",
       " torch.Size([1, 224, 224]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention head\n",
    "torch.manual_seed(5550)\n",
    "\n",
    "query = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "key = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "value = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "# project into seperate spaces\n",
    "q = query(x) # (B, H*W, hidden_dim) \n",
    "k = key(x)\n",
    "v = value(x)\n",
    "\n",
    "# attention scores\n",
    "scores = q @ k.transpose(-2,-1)\n",
    "scores /= math.sqrt(hidden_dim) # print(scores.std()) will be ish 0.4 => breaks gaussian assumption\n",
    "scores = F.softmax(scores, dim = -1)\n",
    "out = scores @ v\n",
    "\n",
    "\n",
    "q.shape, k.transpose(-2,-1).shape, scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c613baa-a7b5-4a65-9e78-baf566b7b630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 224, 256]), torch.Size([1, 224, 256]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5550)\n",
    "nhead = 8\n",
    "hidden_dim = 256 # = d_model in AttentionIsAllYouNeed\n",
    "\n",
    "\n",
    "class EncoderHead(nn.Module):\n",
    "    def __init__(self, positional_encoding, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.head_size = hidden_dim // nhead # 32\n",
    "        self.query = nn.Linear(hidden_dim, self.head_size, bias = False) # 256 -> 32\n",
    "        self.key = nn.Linear(hidden_dim, self.head_size, bias = False)\n",
    "        self.value = nn.Linear(hidden_dim, self.head_size, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # project into separate spaces\n",
    "        q = self.query(x + self.positional_encoding) # (B, H*W, head_size) \n",
    "        k = self.key(x + self.positional_encoding)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # attention scores\n",
    "        scores = q @ k.transpose(-2,-1)\n",
    "        scores /= math.sqrt(self.head_size) # print(scores.std()) will be ish 0.4 => breaks gaussian assumption\n",
    "        scores = F.softmax(scores, dim = -1)\n",
    "        out = scores @ v # (B, H*W, head_size) \n",
    "\n",
    "        return out\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, pos_encode, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.encoder_heads = nn.ModuleList([EncoderHead(pos_encode) for _ in range(nhead)])\n",
    "        self.cat_proj = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cat = torch.cat([head(x) for head in self.encoder_heads], dim = -1) # (B, HW, head_size) -> (B, HW, hidden_dim)\n",
    "        out = self.cat_proj(cat)\n",
    "        return out\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, pos_encode, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.pos_encode = pos_encode\n",
    "        self.encoder_layer = EncoderLayer(self.pos_encode)\n",
    "\n",
    "        # layer norm\n",
    "        self.gamma = nn.Parameter(torch.ones(1, 1, hidden_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        # ffn\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # attention layer\n",
    "        x_res = x\n",
    "        mean = x_res.mean(dim = -1, keepdim = True) # for layernorm\n",
    "        var = x_res.var(dim = -1, correction = 0, keepdim = True)\n",
    "        x_res = x_res - mean\n",
    "        x_res = x_res / torch.sqrt(var + self.eps)\n",
    "        x_res = x_res*self.gamma + self.beta\n",
    "        x_res = self.encoder_layer(x_res) # (B, HW, hidden_dim)\n",
    "        x = x + x_res # residual connection\n",
    "\n",
    "        # compute layer\n",
    "        x_res = x\n",
    "        x_res = self.layer_norm(x_res)\n",
    "        x_res = self.fc1(x_res)\n",
    "        x_res = self.relu(x_res)\n",
    "        x_res = self.fc2(x_res)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, pos_encode, num_encoder_layers = 6, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderBlock(pos_encode) for _ in range(num_encoder_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        \n",
    "pos_encode = nn.Parameter(torch.randn(size = (1, HW, hidden_dim), dtype = flattened.dtype))\n",
    "elayer = EncoderLayer(pos_encode)\n",
    "block = EncoderBlock(pos_encode)\n",
    "encoder = Encoder(pos_encode)\n",
    "encoder(x).shape, x.shape\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6adaf7c-ead1-4027-b90d-b2f283b44378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7841,  0.2449,  1.3215,  0.3390],\n",
      "         [ 0.1875,  1.0745,  0.8846, -0.1295]]])\n",
      "tensor([[[0.7841, 0.2449],\n",
      "         [0.1875, 1.0745]]])\n",
      "tensor([[[ 1.3215,  0.3390],\n",
      "         [ 0.8846, -0.1295]]])\n",
      "torch.Size([2, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(1,2,4)\n",
    "print(t)\n",
    "t = t.split(2, dim = -1)\n",
    "# t[1].shape, len(t)\n",
    "print(t[0])\n",
    "print(t[1])\n",
    "print(torch.stack(t).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8b052b8-15e3-4b08-92b6-c0725b07a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead):\n",
    "        super().__init__()\n",
    "        self.nhead = nhead\n",
    "        self.head_size = embed_dim // nhead\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \n",
    "        B, qT, C = query.shape\n",
    "        _, kT, _ = key.shape\n",
    "        \n",
    "        q = self.query(query)\n",
    "        k = self.key(key)\n",
    "        v = self.value(value)\n",
    "        \n",
    "        # TODO: go over pen and paper for reshaping q, k, v\n",
    "        q = q.view(B, qT, self.nhead, self.head_size).transpose(1,2) # (B, nhead, qT, head_size)\n",
    "        k = k.view(B, kT, self.nhead, self.head_size).transpose(1,2) # (B, nhead, kT, head_size)\n",
    "        v = v.view(B, kT, self.nhead, self.head_size).transpose(1,2)\n",
    "            \n",
    "        scores = q @ k.transpose(-2,-1) # (B, nhead, qT, head_size) @ (B, nhead, head_size, kT) ---> # (B, nhead, qT, kT)\n",
    "        scores /= math.sqrt(self.head_size)\n",
    "        scores = F.softmax(scores, dim = -1)\n",
    "        out = scores @ v # (B, nhead, qT, kT) @ (B, nhead, kT, head_size) ---> (B, nhead, qT, head_size)\n",
    "        out = out.transpose(1,2).flatten(2) # (B, nhead, qT, head_size) ---> (B, qT, nhead, head_size) ---> (B, qT, hidden_dim)\n",
    "        out = self.projection(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# batch_size = 1\n",
    "# encoder = Encoder(pos_encode)\n",
    "# mem = encoder(x)\n",
    "# q = torch.zeros((batch_size,100, hidden_dim))\n",
    "# mhattn = MHAttention(256, 8)\n",
    "# mhattn(q, mem, mem).shape\n",
    "\n",
    "# q.shape, mem.shape\n",
    "\n",
    "# pos_encode = nn.Parameter(torch.randn(size = (1, HW, hidden_dim), dtype = flattened.dtype))\n",
    "# mhattn = MHAttention(256, 8)\n",
    "# q = x + pos_encode\n",
    "# k = x + pos_encode\n",
    "# v = x\n",
    "# mhattn(q,k,v).shape\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee336c00-4e73-4c99-9c4c-e96a6adf3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the decoder\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.multihead_selfattention = MHAttention(embed_dim = hidden_dim, nhead = nhead)\n",
    "        self.layer_norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.multihead_crossattention = MHAttention(embed_dim = hidden_dim, nhead = nhead)\n",
    "        self.layer_norm_3 = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = nn.Sequential(nn.Linear(hidden_dim, hidden_dim*4), nn.ReLU(), nn.Linear(hidden_dim*4, hidden_dim)) \n",
    "\n",
    "        \n",
    "    def forward(self, x, memory, memory_pos_encoding, query_pos_encoding):\n",
    "        # self-attention\n",
    "        x_res = self.layer_norm_1(x)\n",
    "        q = x_res + query_pos_encoding\n",
    "        k = x_res + query_pos_encoding\n",
    "        v = x_res \n",
    "        x_res = self.multihead_selfattention(query = q, key = k, value = v)\n",
    "        x = x + x_res\n",
    "\n",
    "        # cross-attention\n",
    "        x_res = self.layer_norm_2(x)\n",
    "        q = x_res + query_pos_encoding\n",
    "        k = memory + memory_pos_encoding\n",
    "        v = memory\n",
    "        x_res = self.multihead_crossattention(query = q, key = k, value = v)\n",
    "        x = x + x_res\n",
    "\n",
    "        #ffn\n",
    "        x_res = self.layer_norm_3(x)\n",
    "        x_res = self.ffn(x_res)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n",
    "\n",
    "# batch_size = 1\n",
    "# encoder = Encoder(pos_encode)\n",
    "\n",
    "# mem = encoder(x)\n",
    "# q_pos = nn.Parameter(torch.zeros((batch_size,100, hidden_dim)))\n",
    "\n",
    "# decoderblock = DecoderBlock(memory = mem, memory_pos_encoding = pos_encode, query_pos_encoding = q_pos)\n",
    "# q = torch.zeros(100, hidden_dim)\n",
    "# decoderblock(q).shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aedce328-244a-4e8e-bc36-d4db56e3b34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class nanoDETR(nn.Module):\n",
    "    def __init__(self, resnet50, ntokens = 224, nlayers = 6, nhead = 8, hidden_dim = 256, nqueries = 100):\n",
    "        super().__init__()\n",
    "        self.nlayers = nlayers\n",
    "        self.nhead = nhead\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # backbone\n",
    "        self.resnet50 = resnet50\n",
    "        self.backbone = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "        self.project = nn.Conv2d(in_channels = 2048, out_channels = hidden_dim, kernel_size = 1, bias=False)\n",
    "        \n",
    "        # build encoder\n",
    "        self.encoder_pos_encode = nn.Parameter(torch.randn(ntokens, hidden_dim))\n",
    "        self.encoder = Encoder(self.encoder_pos_encode, num_encoder_layers = nlayers, hidden_dim = hidden_dim, nhead = nhead)\n",
    "\n",
    "        # build decoder\n",
    "        self.nqueries = nqueries\n",
    "        self.query_pos_encoding = nn.Parameter(torch.randn((nqueries, hidden_dim)))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # expect x to be raw img from resnet50, e.g. x.shape = (B, C, H, W) = (1, 2048, 14, 16)\n",
    "        # we assume ntokens when we overfit to one img to check for bugs. It will be made general => cannot use fixed-size feat.dim embedding\n",
    "        assert x.shape == (3, 442, 500), f'image shape is {x.shape}, but should be (3, 442, 500)'\n",
    "\n",
    "        # backbone\n",
    "        with torch.no_grad():\n",
    "            x = self.backbone(x.unsqueeze(0))\n",
    "        x = self.project(x)\n",
    "        x = x.flatten(2) # (B, hidden_dim, T)\n",
    "        x = x.transpose(-2,-1) # (B, T, hidden_dim)\n",
    "        # TODO: call contiguous? \n",
    "\n",
    "        # encoder\n",
    "        memory = self.encoder(x)\n",
    "\n",
    "        # decoder\n",
    "        queries = torch.zeros((1, self.nqueries, self.hidden_dim)) # TODO: softcode this\n",
    "        layers = nn.ModuleList([DecoderBlock(hidden_dim = self.hidden_dim, nhead = self.nhead) for _ in range(self.nlayers)])\n",
    "        for layer in layers:\n",
    "            queries = layer(x = queries, \n",
    "                            memory = memory, \n",
    "                            memory_pos_encoding = self.encoder_pos_encode, \n",
    "                            query_pos_encoding = self.query_pos_encoding)\n",
    "\n",
    "        # prediction\n",
    "        return queries\n",
    "        \n",
    "\n",
    "img,_ = dataset[0]\n",
    "detr = nanoDETR(resnet50 = resnet50)\n",
    "detr(img).shape\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a75c35-df25-4736-b116-198258f6df4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
