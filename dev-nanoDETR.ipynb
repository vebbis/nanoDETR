{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25b071e6-9f08-4862-9f5f-0cedd831886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights  \n",
    "import utils\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cd96a7e-2cf7-4864-b899-ea87e884c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = models.resnet50(weights = ResNet50_Weights.IMAGENET1K_V2)\n",
    "backbone = nn.Sequential(*list(backbone.children())[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7fbf9a3d-8b78-408e-a7de-25f6bdd316f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 224, 256])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C, H0, W0 = 3, 442, 500\n",
    "img = torch.randn((C, H0, W0))\n",
    "img.shape\n",
    "img = img.unsqueeze(0)\n",
    "img.shape\n",
    "img = backbone(img)\n",
    "img.shape\n",
    "dmodel = 256\n",
    "proj = nn.Conv2d(2048, 256, 1)\n",
    "img = proj(img)\n",
    "_, _, H, W = img.shape\n",
    "img = img.flatten(-2,-1) # (B, dmodel, ntoken)\n",
    "img.shape\n",
    "img = img.transpose(-2,-1)\n",
    "img.shape # (B, ntokens, dmodel)\n",
    "\n",
    "pos_encode = utils.sinusoidal_pos_encode_2d(max_dim = 100, d_model = 256)\n",
    "pos_encode = pos_encode[:H, :W]\n",
    "pos_encode.shape\n",
    "pos_encode = pos_encode.flatten(0,1)\n",
    "pos_encode.shape\n",
    "pos_encode = pos_encode.unsqueeze(0)\n",
    "pos_encode.shape, img.shape # (B, ntokens, dmodel)\n",
    "\n",
    "query = nn.Linear(dmodel, dmodel, bias = False)\n",
    "key = nn.Linear(dmodel, dmodel, bias = False)\n",
    "value = nn.Linear(dmodel, dmodel, bias = False)\n",
    "\n",
    "q = query(img + pos_encode)\n",
    "k = key(img + pos_encode)\n",
    "v = value(img)\n",
    "q.shape, k.transpose(-2,-1).shape\n",
    "scores = q @ k.transpose(-2,-1)\n",
    "scores.shape\n",
    "scores /= math.sqrt(dmodel)\n",
    "scores = scores.softmax(dim = -1)\n",
    "out = scores @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f64c9ba7-e21e-40c3-acd9-092ca0c75766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderHead(nn.Module):\n",
    "    def __init__(self, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.head_size = hidden_dim // nhead # 32\n",
    "        self.query = nn.Linear(hidden_dim, self.head_size, bias = False) # 256 -> 32\n",
    "        self.key = nn.Linear(hidden_dim, self.head_size, bias = False)\n",
    "        self.value = nn.Linear(hidden_dim, self.head_size, bias = False)\n",
    "        \n",
    "    def forward(self, x, positional_encoding):\n",
    "        # project into separate spaces\n",
    "        q = self.query(x + positional_encoding) # (B, H*W, head_size) \n",
    "        k = self.key(x + positional_encoding)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # attention scores\n",
    "        scores = q @ k.transpose(-2,-1)\n",
    "        scores /= math.sqrt(self.head_size) \n",
    "        scores = F.softmax(scores, dim = -1)\n",
    "        out = scores @ v # (B, H*W, head_size) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "324effc8-c8d6-4c6c-8de5-ba2ef5e79dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 224, 256])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhead = 8\n",
    "eheads = [EncoderHead() for _ in range(nhead)]\n",
    "vals = [eh(img, pos_encode) for head in eheads]\n",
    "vals = torch.cat(vals, dim = -1)\n",
    "final_proj = nn.Linear(dmodel, dmodel)\n",
    "vals = final_proj(vals)\n",
    "vals.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4fca616e-2f1b-4e00-bdae-de0c3cf9beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.encoder_heads = nn.ModuleList([EncoderHead() for _ in range(nhead)])\n",
    "        self.cat_proj = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "    def forward(self, x, positional_encoding):\n",
    "        cat = torch.cat([head(x, positional_encoding) for head in self.encoder_heads], dim = -1) # (B, HW, head_size) -> (B, HW, hidden_dim)\n",
    "        out = self.cat_proj(cat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b4157a86-d536-4237-950d-dc3fd56ecf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 224, 256]), torch.Size([1, 224, 256]))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn1 = nn.Linear(dmodel, dmodel*4)\n",
    "relu = nn.ReLU()\n",
    "ffn2 = nn.Linear(dmodel*4, dmodel)\n",
    "ln1 = nn.LayerNorm(dmodel)\n",
    "ln2 = nn.LayerNorm(dmodel)\n",
    "el = EncoderLayer()\n",
    "\n",
    "x = img\n",
    "x_res = x\n",
    "x_res = ln1(x_res)\n",
    "x_res = el(x_res, pos_encode)\n",
    "x = x + x_res\n",
    "\n",
    "x_res = x\n",
    "x_res = ln2(x_res)\n",
    "x_res = ffn1(x_res)\n",
    "x_res = relu(x_res)\n",
    "x_res = ffn2(x_res)\n",
    "x = x_res + x\n",
    "img.shape, x.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2c208cdf-6b5b-4f0e-89d5-1742e41939f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = EncoderLayer(hidden_dim= hidden_dim, nhead = nhead)\n",
    "\n",
    "        # layer norm\n",
    "        self.gamma = nn.Parameter(torch.ones(1, 1, hidden_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        # ffn\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim)\n",
    "\n",
    "    def forward(self, x, positional_encoding):\n",
    "        # attention layer\n",
    "        x_res = x\n",
    "        mean = x_res.mean(dim = -1, keepdim = True) # for layernorm\n",
    "        var = x_res.var(dim = -1, correction = 0, keepdim = True)\n",
    "        x_res = x_res - mean\n",
    "        x_res = x_res / torch.sqrt(var + self.eps)\n",
    "        x_res = x_res*self.gamma + self.beta\n",
    "        x_res = self.encoder_layer(x_res, positional_encoding) # (B, HW, hidden_dim)\n",
    "        x = x + x_res # residual connection\n",
    "\n",
    "        # compute layer\n",
    "        x_res = x\n",
    "        x_res = self.layer_norm(x_res)\n",
    "        x_res = self.fc1(x_res)\n",
    "        x_res = self.relu(x_res)\n",
    "        x_res = self.fc2(x_res)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a2fa9701-4622-4f62-9b92-ec63471911a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 224, 256])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = [EncoderBlock() for _ in range(6)]\n",
    "x = img\n",
    "for block in blocks:\n",
    "    img = block(img, pos_encode)\n",
    "\n",
    "x.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcc739-c1c5-4f4f-8b69-b1f36d35811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_encoder_layers = 6, hidden_dim = 256, nhead = 8):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(hidden_dim=hidden_dim, nhead=nhead) for _ in range(num_encoder_layers)])\n",
    "\n",
    "    def forward(self, x, positional_encoding):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, positional_encoding)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "332f9c7d-baad-4046-ac87-1f3a3157756c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 256])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nqueries = 100\n",
    "q = torch.randn((1, nqueries, dmodel))\n",
    "k = v = x\n",
    "q.shape, k.shape, v.shape\n",
    "\n",
    "query = nn.Linear(dmodel, dmodel, bias = False)\n",
    "key = nn.Linear(dmodel, dmodel, bias = False)\n",
    "value = nn.Linear(dmodel, dmodel, bias = False)\n",
    "\n",
    "q = query(q)\n",
    "k = key(k)\n",
    "v = value(v)\n",
    "q.shape # (B, nqueries, dmodel)\n",
    "k.shape, v.shape # (B, ntokens, dmodel)\n",
    "\n",
    "nhead = 8\n",
    "hs = dmodel // nhead # 32\n",
    "\n",
    "# q = q.view(1,nqueries,nhead,hs)\n",
    "# q.shape # (B, nqueries, nhead, hs)\n",
    "# q = q.transpose(1,2)\n",
    "# q.shape # (B, nhead, nqueries, hs)\n",
    "\n",
    "ntokens = k.shape[1]\n",
    "q = q.view(1,nqueries,nhead,hs).transpose(1,2) # (B, nhead, nqueries, hs)\n",
    "k = k.view(1,ntokens, nhead, hs).transpose(1,2) # (B, nhead, ntokens, hs)\n",
    "v = v.view(1,ntokens, nhead, hs).transpose(1,2) # (B, nhead, ntokens, hs)\n",
    "\n",
    "scores = q @ k.transpose(-2,-1) \n",
    "# (B, nhead, nqueries, hs) @ (B, nhead, hs, ntokens)\n",
    "# ---> (B, nhead, nqueries, ntokens)\n",
    "scores /= math.sqrt(hs)\n",
    "scores = scores.softmax(dim = -1) # (B, nhead, nqueries, ntokens)\n",
    "out = scores @ v\n",
    " # (B, nhead, nqueries, ntokens) @ (B, nhead, ntokens, hs)\n",
    "# (B, nhead, nqueries, hs)\n",
    "\n",
    "# would like (B, nqueries, dmodel)\n",
    "out = out.transpose(1,2)\n",
    "out.shape # (B, nqueries, nhead, hs)\n",
    "out = out.flatten(-2,-1)\n",
    "out.shape\n",
    "final_transf = nn.Linear(dmodel, dmodel, bias = False)\n",
    "out = final_transf(out)\n",
    "out.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ac0401a3-e56b-488a-a36f-1096de5a0e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]),\n",
       " tensor([[0, 1],\n",
       "         [2, 3]]))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(4)\n",
    "a, a.view(2,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
